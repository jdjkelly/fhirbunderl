{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mAudited \u001b[1m2 packages\u001b[0m \u001b[2min 12ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install dotenv \"numpy<2.0.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mAudited \u001b[1m2 packages\u001b[0m \u001b[2min 58ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install openpipe-art openpipe --prerelease allow --no-cache-dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Optional\n",
    "WANDB_API_KEY = \"\"\n",
    "if WANDB_API_KEY:\n",
    "    os.environ[\"WANDB_API_KEY\"] = WANDB_API_KEY\n",
    "\n",
    "# Optional\n",
    "OPENPIPE_API_KEY = \"\"\n",
    "if OPENPIPE_API_KEY:\n",
    "    os.environ[\"OPENPIPE_API_KEY\"] = OPENPIPE_API_KEY\n",
    "    \n",
    "MODEL_NAME = \"001\"\n",
    "PROJECT = \"generate-fhir-single-turn\"\n",
    "BASE_MODEL = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "LEARNING_RATE = 1.2e-5\n",
    "GROUPS_PER_STEP = 1\n",
    "EVAL_STEPS = 50\n",
    "VAL_SET_SIZE = 100\n",
    "NUM_EPOCHS = 1\n",
    "NUM_GENERATIONS = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS available, using Apple Silicon GPU\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 47\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Create model with CPU-friendly configuration\u001b[39;00m\n\u001b[32m     22\u001b[39m model = art.TrainableModel(\n\u001b[32m     23\u001b[39m     name=MODEL_NAME,\n\u001b[32m     24\u001b[39m     project=PROJECT,\n\u001b[32m   (...)\u001b[39m\u001b[32m     45\u001b[39m     ),\n\u001b[32m     46\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m model.register(backend)\n\u001b[32m     48\u001b[39m op_client = AsyncOpenPipe(api_key=os.getenv(\u001b[33m\"\u001b[39m\u001b[33mOPENPIPE_API_KEY\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m     50\u001b[39m \u001b[38;5;66;03m## Load the training data\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/firestation/.venv/lib/python3.11/site-packages/art/model.py:190\u001b[39m, in \u001b[36mTrainableModel.register\u001b[39m\u001b[34m(self, backend, _openai_client_config)\u001b[39m\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mregister\u001b[39m(\n\u001b[32m    185\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    186\u001b[39m     backend: \u001b[33m\"\u001b[39m\u001b[33mBackend\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    187\u001b[39m     _openai_client_config: dev.OpenAIServerConfig | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    188\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    189\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().register(backend)\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     base_url, api_key = \u001b[38;5;28;01mawait\u001b[39;00m backend._prepare_backend_for_training(\n\u001b[32m    191\u001b[39m         \u001b[38;5;28mself\u001b[39m, _openai_client_config\n\u001b[32m    192\u001b[39m     )\n\u001b[32m    194\u001b[39m     \u001b[38;5;66;03m# Populate the new top-level inference fields so that the rest of the\u001b[39;00m\n\u001b[32m    195\u001b[39m     \u001b[38;5;66;03m# code (and any user code) can create an OpenAI client immediately.\u001b[39;00m\n\u001b[32m    196\u001b[39m     \u001b[38;5;28mself\u001b[39m.inference_base_url = base_url\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/firestation/.venv/lib/python3.11/site-packages/art/local/backend.py:219\u001b[39m, in \u001b[36mLocalBackend._prepare_backend_for_training\u001b[39m\u001b[34m(self, model, config)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_prepare_backend_for_training\u001b[39m(\n\u001b[32m    215\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    216\u001b[39m     model: TrainableModel,\n\u001b[32m    217\u001b[39m     config: dev.OpenAIServerConfig | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    218\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m     service = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._get_service(model)\n\u001b[32m    220\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m service.start_openai_server(config=config)\n\u001b[32m    221\u001b[39m     server_args = (config \u001b[38;5;129;01mor\u001b[39;00m {}).get(\u001b[33m\"\u001b[39m\u001b[33mserver_args\u001b[39m\u001b[33m\"\u001b[39m, {})\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/firestation/.venv/lib/python3.11/site-packages/art/local/backend.py:111\u001b[39m, in \u001b[36mLocalBackend._get_service\u001b[39m\u001b[34m(self, model)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_service\u001b[39m(\u001b[38;5;28mself\u001b[39m, model: TrainableModel) -> ModelService:\n\u001b[32m    110\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model.name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._services:\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m         config = \u001b[43mdev\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_model_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m            \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_model_dir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mart_path\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_internal_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    116\u001b[39m         \u001b[38;5;28mself\u001b[39m._services[model.name] = ModelService(\n\u001b[32m    117\u001b[39m             host=\u001b[33m\"\u001b[39m\u001b[33mlocalhost\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    118\u001b[39m             port=\u001b[32m8089\u001b[39m + \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._services),\n\u001b[32m   (...)\u001b[39m\u001b[32m    122\u001b[39m             output_dir=get_model_dir(model=model, art_path=\u001b[38;5;28mself\u001b[39m._path),\n\u001b[32m    123\u001b[39m         )\n\u001b[32m    124\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._in_process:\n\u001b[32m    125\u001b[39m             \u001b[38;5;66;03m# Kill all \"model-service\" processes to free up GPU memory\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/firestation/.venv/lib/python3.11/site-packages/art/dev/model.py:44\u001b[39m, in \u001b[36mget_model_config\u001b[39m\u001b[34m(base_model, output_dir, config)\u001b[39m\n\u001b[32m     25\u001b[39m enable_sleep_mode = config.get(\u001b[33m\"\u001b[39m\u001b[33mengine_args\u001b[39m\u001b[33m\"\u001b[39m, {}).get(\u001b[33m\"\u001b[39m\u001b[33menable_sleep_mode\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     26\u001b[39m init_args = InitArgs(\n\u001b[32m     27\u001b[39m     model_name=base_model,\n\u001b[32m     28\u001b[39m     max_seq_length=\u001b[32m32768\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     38\u001b[39m     use_async=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     39\u001b[39m )\n\u001b[32m     40\u001b[39m engine_args = EngineArgs(\n\u001b[32m     41\u001b[39m     disable_log_requests=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     42\u001b[39m     \u001b[38;5;66;03m# Multi-step processing is not supported for the Xformers attention backend\u001b[39;00m\n\u001b[32m     43\u001b[39m     \u001b[38;5;66;03m# which is the fallback for devices with compute capability < 8.0\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m     num_scheduler_steps=\u001b[32m16\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_device_capability\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m] >= \u001b[32m8\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m1\u001b[39m,\n\u001b[32m     45\u001b[39m     enable_sleep_mode=enable_sleep_mode,\n\u001b[32m     46\u001b[39m     generation_config=\u001b[33m\"\u001b[39m\u001b[33mvllm\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     47\u001b[39m )\n\u001b[32m     48\u001b[39m engine_args.update(config.get(\u001b[33m\"\u001b[39m\u001b[33mengine_args\u001b[39m\u001b[33m\"\u001b[39m, {}))\n\u001b[32m     49\u001b[39m init_args.update(config.get(\u001b[33m\"\u001b[39m\u001b[33minit_args\u001b[39m\u001b[33m\"\u001b[39m, {}))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/firestation/.venv/lib/python3.11/site-packages/torch/cuda/__init__.py:507\u001b[39m, in \u001b[36mget_device_capability\u001b[39m\u001b[34m(device)\u001b[39m\n\u001b[32m    494\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_device_capability\u001b[39m(device: Optional[_device_t] = \u001b[38;5;28;01mNone\u001b[39;00m) -> Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]:\n\u001b[32m    495\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Get the cuda capability of a device.\u001b[39;00m\n\u001b[32m    496\u001b[39m \n\u001b[32m    497\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    505\u001b[39m \u001b[33;03m        tuple(int, int): the major and minor cuda capability of the device\u001b[39;00m\n\u001b[32m    506\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m507\u001b[39m     prop = \u001b[43mget_device_properties\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    508\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m prop.major, prop.minor\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/firestation/.venv/lib/python3.11/site-packages/torch/cuda/__init__.py:523\u001b[39m, in \u001b[36mget_device_properties\u001b[39m\u001b[34m(device)\u001b[39m\n\u001b[32m    511\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_device_properties\u001b[39m(device: Optional[_device_t] = \u001b[38;5;28;01mNone\u001b[39;00m) -> _CudaDeviceProperties:\n\u001b[32m    512\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Get the properties of a device.\u001b[39;00m\n\u001b[32m    513\u001b[39m \n\u001b[32m    514\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    521\u001b[39m \u001b[33;03m        _CudaDeviceProperties: the properties of the device\u001b[39;00m\n\u001b[32m    522\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m523\u001b[39m     \u001b[43m_lazy_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# will define _get_device_properties\u001b[39;00m\n\u001b[32m    524\u001b[39m     device = _get_device_index(device, optional=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    525\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m device < \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m device >= device_count():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/firestation/.venv/lib/python3.11/site-packages/torch/cuda/__init__.py:310\u001b[39m, in \u001b[36m_lazy_init\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    305\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    306\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    307\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmultiprocessing, you must use the \u001b[39m\u001b[33m'\u001b[39m\u001b[33mspawn\u001b[39m\u001b[33m'\u001b[39m\u001b[33m start method\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    308\u001b[39m     )\n\u001b[32m    309\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch._C, \u001b[33m\"\u001b[39m\u001b[33m_cuda_getDeviceCount\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mTorch not compiled with CUDA enabled\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    311\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    312\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[32m    313\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    314\u001b[39m     )\n",
      "\u001b[31mAssertionError\u001b[39m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import art\n",
    "from art.local import LocalBackend\n",
    "from openpipe import AsyncOpenPipe\n",
    "\n",
    "backend = LocalBackend()\n",
    "model = art.TrainableModel(\n",
    "    name=MODEL_NAME,\n",
    "    project=PROJECT,\n",
    "    base_model=BASE_MODEL,\n",
    "    _internal_config=art.dev.InternalModelConfig(\n",
    "        init_args=art.dev.InitArgs(\n",
    "            gpu_memory_utilization=0.75,\n",
    "        ),\n",
    "        peft_args=art.dev.PeftArgs(\n",
    "            lora_alpha=8,\n",
    "        ),\n",
    "        trainer_args=art.dev.TrainerArgs(\n",
    "            max_grad_norm=0.1,\n",
    "        ),\n",
    "    ),\n",
    ")\n",
    "await model.register(backend)\n",
    "op_client = AsyncOpenPipe(api_key=os.getenv(\"OPENPIPE_API_KEY\"))\n",
    "\n",
    "## Load the training data\n",
    "print(\"Loading training data...\")\n",
    "train_dataset: datasets.Dataset = datasets.load_dataset(\"../data/fhir-single-turn\")\n",
    "\n",
    "train_data_list: List[Dict[str, Any]] = list(train_dataset)  # type: ignore\n",
    "print(f\"Training data size: {len(train_data_list)}\")\n",
    "\n",
    "# Get OpenAI Client for the ART Model\n",
    "openai_client = model.openai_client()\n",
    "\n",
    "# Training Loop\n",
    "start_step = await model.get_step()\n",
    "print(f\"Starting training from global step {start_step}\")\n",
    "\n",
    "data_iterator = art.utils.iterate_dataset(\n",
    "    dataset=train_data_list,\n",
    "    groups_per_step=GROUPS_PER_STEP,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    initial_step=start_step,\n",
    "    use_tqdm=True,\n",
    ")\n",
    "\n",
    "for batch_inputs, epoch, global_step, epoch_step in data_iterator:\n",
    "  print(batch_inputs)\n",
    "  print(epoch)\n",
    "  print(global_step)\n",
    "  print(epoch_step)\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
